{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niter 0\n",
      "niter 1c regression: Gradient Descent(0/99): loss=173286.79513998624\n",
      "niter 2c regression: Gradient Descent(1/99): loss=-9152561.105640735\n",
      "niter 3c regression: Gradient Descent(2/99): loss=-13301203.086940361\n",
      "niter 4c regression: Gradient Descent(3/99): loss=-17439933.04763793\n",
      "niter 5c regression: Gradient Descent(4/99): loss=-21574066.220491875\n",
      "niter 6c regression: Gradient Descent(5/99): loss=-25705923.60566914\n",
      "niter 7c regression: Gradient Descent(6/99): loss=-29836529.557113007\n",
      "niter 8c regression: Gradient Descent(7/99): loss=-33966381.876006246\n",
      "niter 9c regression: Gradient Descent(8/99): loss=-38095746.54052388\n",
      "niter 10 regression: Gradient Descent(9/99): loss=-42224777.59631214\n",
      "niter 11 regression: Gradient Descent(10/99): loss=-46353570.23441997\n",
      "niter 12 regression: Gradient Descent(11/99): loss=-50482186.38601099\n",
      "niter 13 regression: Gradient Descent(12/99): loss=-54610668.069024205\n",
      "niter 14 regression: Gradient Descent(13/99): loss=-58739044.80634714\n",
      "niter 15 regression: Gradient Descent(14/99): loss=-62867337.964536615\n",
      "niter 16 regression: Gradient Descent(15/99): loss=-66995563.40333573\n",
      "niter 17 regression: Gradient Descent(16/99): loss=-71123733.1551119\n",
      "niter 18 regression: Gradient Descent(17/99): loss=-75251856.5241011\n",
      "niter 19 regression: Gradient Descent(18/99): loss=-79379940.82614206\n",
      "niter 20 regression: Gradient Descent(19/99): loss=-83507991.8987744\n",
      "niter 21 regression: Gradient Descent(20/99): loss=-87636014.46075846\n",
      "niter 22 regression: Gradient Descent(21/99): loss=-91764012.37054573\n",
      "niter 23 regression: Gradient Descent(22/99): loss=-95891988.81550673\n",
      "niter 24 regression: Gradient Descent(23/99): loss=-100019946.4527945\n",
      "niter 25 regression: Gradient Descent(24/99): loss=-104147887.51581681\n",
      "niter 26 regression: Gradient Descent(25/99): loss=-108275813.8958406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucien/Documents/machinelearning/mlproj1/MLproj1/scripts/implementations.py:46: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(t)/(np.exp(t)+1)\n",
      "/Users/lucien/Documents/machinelearning/mlproj1/MLproj1/scripts/implementations.py:46: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(t)/(np.exp(t)+1)\n",
      "/Users/lucien/Documents/machinelearning/mlproj1/MLproj1/scripts/implementations.py:38: RuntimeWarning: overflow encountered in exp\n",
      "  return np.log(1+np.exp(tx.dot(w))).sum()-y.T.dot(tx).dot(w).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niter 27 regression: Gradient Descent(26/99): loss=inf\n",
      "niter 28 regression: Gradient Descent(27/99): loss=nan\n",
      "niter 29 regression: Gradient Descent(28/99): loss=nan\n",
      "niter 30 regression: Gradient Descent(29/99): loss=nan\n",
      "niter 31 regression: Gradient Descent(30/99): loss=nan\n",
      "niter 32 regression: Gradient Descent(31/99): loss=nan\n",
      "niter 33 regression: Gradient Descent(32/99): loss=nan\n",
      "niter 34 regression: Gradient Descent(33/99): loss=nan\n",
      "niter 35 regression: Gradient Descent(34/99): loss=nan\n",
      "niter 36 regression: Gradient Descent(35/99): loss=nan\n",
      "niter 37 regression: Gradient Descent(36/99): loss=nan\n",
      "niter 38 regression: Gradient Descent(37/99): loss=nan\n",
      "niter 39 regression: Gradient Descent(38/99): loss=nan\n",
      "niter 40 regression: Gradient Descent(39/99): loss=nan\n",
      "niter 41 regression: Gradient Descent(40/99): loss=nan\n",
      "niter 42 regression: Gradient Descent(41/99): loss=nan\n",
      "niter 43 regression: Gradient Descent(42/99): loss=nan\n",
      "niter 44 regression: Gradient Descent(43/99): loss=nan\n",
      "niter 45 regression: Gradient Descent(44/99): loss=nan\n",
      "niter 46 regression: Gradient Descent(45/99): loss=nan\n",
      "niter 47 regression: Gradient Descent(46/99): loss=nan\n",
      "niter 48 regression: Gradient Descent(47/99): loss=nan\n",
      "niter 49 regression: Gradient Descent(48/99): loss=nan\n",
      "niter 50 regression: Gradient Descent(49/99): loss=nan\n",
      "niter 51 regression: Gradient Descent(50/99): loss=nan\n",
      "niter 52 regression: Gradient Descent(51/99): loss=nan\n",
      "niter 53 regression: Gradient Descent(52/99): loss=nan\n",
      "niter 54 regression: Gradient Descent(53/99): loss=nan\n",
      "niter 55 regression: Gradient Descent(54/99): loss=nan\n",
      "niter 56 regression: Gradient Descent(55/99): loss=nan\n",
      "niter 57 regression: Gradient Descent(56/99): loss=nan\n",
      "niter 58 regression: Gradient Descent(57/99): loss=nan\n",
      "niter 59 regression: Gradient Descent(58/99): loss=nan\n",
      "niter 60 regression: Gradient Descent(59/99): loss=nan\n",
      "niter 61 regression: Gradient Descent(60/99): loss=nan\n",
      "niter 62 regression: Gradient Descent(61/99): loss=nan\n",
      "niter 63 regression: Gradient Descent(62/99): loss=nan\n",
      "niter 64 regression: Gradient Descent(63/99): loss=nan\n",
      "niter 65 regression: Gradient Descent(64/99): loss=nan\n",
      "niter 66 regression: Gradient Descent(65/99): loss=nan\n",
      "niter 67 regression: Gradient Descent(66/99): loss=nan\n",
      "niter 68 regression: Gradient Descent(67/99): loss=nan\n",
      "niter 69 regression: Gradient Descent(68/99): loss=nan\n",
      "niter 70 regression: Gradient Descent(69/99): loss=nan\n",
      "niter 71 regression: Gradient Descent(70/99): loss=nan\n",
      "niter 72 regression: Gradient Descent(71/99): loss=nan\n",
      "niter 73 regression: Gradient Descent(72/99): loss=nan\n",
      "niter 74 regression: Gradient Descent(73/99): loss=nan\n",
      "niter 75 regression: Gradient Descent(74/99): loss=nan\n",
      "niter 76 regression: Gradient Descent(75/99): loss=nan\n",
      "niter 77 regression: Gradient Descent(76/99): loss=nan\n",
      "niter 78 regression: Gradient Descent(77/99): loss=nan\n",
      "niter 79 regression: Gradient Descent(78/99): loss=nan\n",
      "niter 80 regression: Gradient Descent(79/99): loss=nan\n",
      "niter 81 regression: Gradient Descent(80/99): loss=nan\n",
      "niter 82 regression: Gradient Descent(81/99): loss=nan\n",
      "niter 83 regression: Gradient Descent(82/99): loss=nan\n",
      "niter 84 regression: Gradient Descent(83/99): loss=nan\n",
      "niter 85 regression: Gradient Descent(84/99): loss=nan\n",
      "niter 86 regression: Gradient Descent(85/99): loss=nan\n",
      "niter 87 regression: Gradient Descent(86/99): loss=nan\n",
      "niter 88 regression: Gradient Descent(87/99): loss=nan\n",
      "niter 89 regression: Gradient Descent(88/99): loss=nan\n",
      "niter 90 regression: Gradient Descent(89/99): loss=nan\n",
      "niter 91 regression: Gradient Descent(90/99): loss=nan\n",
      "niter 92 regression: Gradient Descent(91/99): loss=nan\n",
      "niter 93 regression: Gradient Descent(92/99): loss=nan\n",
      "niter 94 regression: Gradient Descent(93/99): loss=nan\n",
      "niter 95 regression: Gradient Descent(94/99): loss=nan\n",
      "niter 96 regression: Gradient Descent(95/99): loss=nan\n",
      "niter 97 regression: Gradient Descent(96/99): loss=nan\n",
      "niter 98 regression: Gradient Descent(97/99): loss=nan\n",
      "niter 99 regression: Gradient Descent(98/99): loss=nan\n",
      "nanistic regression: Gradient Descent(99/99): loss=nan\n"
     ]
    }
   ],
   "source": [
    "mse,weights=least_squares(y,tX)\n",
    "\n",
    "w_initial = np.zeros(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares Gradient Descent implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares Stochastic Gradient Descent implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "weights, loss= logistic_regression(y, tX, w_initial, 100, 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularised logistic regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_=1e7\n",
    "weights, loss= reg_logistic_regression(y, tX,lambda_, w_initial, 100, 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
