{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from implementations import *\n",
    "from cross_validation import *\n",
    "from data_preprocessing import *\n",
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data\n",
      "\n",
      "training data loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('loading training data'+\"\\n\")\n",
    "DATA_TEST_PATH = '../data/train.csv'\n",
    "y,tX,ids = load_csv_data(DATA_TEST_PATH)\n",
    "print('training data loaded'+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares Gradient Descent implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: 0\n",
      "test error: 0.49993509424190474\n",
      "train error: 0.4999357248800381\n",
      "accuracy: 0.7843959563607246\n",
      "Subset: 1\n",
      "test error: 0.4999602518791192\n",
      "train error: 0.49996061263993086\n",
      "accuracy: 0.6949316481815837\n",
      "Subset: 2\n",
      "test error: 0.4999362682649859\n",
      "train error: 0.49993685599111737\n",
      "accuracy: 0.6948717948717948\n"
     ]
    }
   ],
   "source": [
    "jet_tX = jet(tX)\n",
    "\n",
    "means = []\n",
    "devs = []\n",
    "degree = [1, 1, 1]\n",
    "# cleans -999 and standardizes\n",
    "for i in range(len(jet_tX)):\n",
    "    print(\"Subset: \" + str(i))\n",
    "    # preprocess every train subset\n",
    "    preprocessed_tX = preprocess_data(tX[jet_tX[i]])\n",
    "    acc, testloss, trainloss, weights = cross_validation_for_GD(y[jet_tX[i]], preprocessed_tX, degree[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares Stochastic Gradient Descent implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: 0\n",
      "test error: 0.4999545349554637\n",
      "train error: 0.49989188664237744\n",
      "accuracy: 0.7312080872785508\n",
      "Subset: 1\n",
      "test error: 0.4999759461830184\n",
      "train error: 0.49997919991484263\n",
      "accuracy: 0.6701057518700027\n",
      "Subset: 2\n",
      "test error: 0.4999471532164\n",
      "train error: 0.499888164400482\n",
      "accuracy: 0.6818169285911222\n"
     ]
    }
   ],
   "source": [
    "jet_tX = jet(tX)\n",
    "\n",
    "means = []\n",
    "devs = []\n",
    "degree = [1, 1, 1]\n",
    "# cleans -999 and standardizes\n",
    "for i in range(len(jet_tX)):\n",
    "    print(\"Subset: \" + str(i))\n",
    "    # preprocess every train subset\n",
    "    preprocessed_tX = preprocess_data(tX[jet_tX[i]])\n",
    "    acc, testloss, trainloss, weights = cross_validation_for_GD(y[jet_tX[i]], preprocessed_tX, degree[i], stoch = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset: 0\n",
      "test error: 34179422.74176813\n",
      "train error: 0.2410241456261793\n",
      "accuracy: 0.8325292763487138\n",
      "subset: 1\n",
      "test error: 0.3345579466053833\n",
      "train error: 0.3221744551437434\n",
      "accuracy: 0.775960794428682\n",
      "subset: 2\n",
      "test error: 90222.0608500605\n",
      "train error: 0.5801349865593991\n",
      "accuracy: 0.7769644334160463\n"
     ]
    }
   ],
   "source": [
    "jet_tX = jet(tX)\n",
    "\n",
    "means = []\n",
    "devs = []\n",
    "degrees = [4,4,11]\n",
    "# cleans -999 and standardizes\n",
    "for i in range(len(jet_tX)):\n",
    "    # preprocess every train subset\n",
    "    preprocessed_tX = preprocess_data(tX[jet_tX[i]])\n",
    "    print(\"subset: \" + str(i))\n",
    "\n",
    "    acc, testloss, trainloss, weights = cross_validation_for_leastsquares(y[jet_tX[i]], preprocessed_tX, degrees[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: 0\n",
      "test error: 1.310991676774038e+35\n",
      "train error: 0.22984703002509174\n",
      "accuracy: 0.8435191672505254\n",
      "Subset: 1\n",
      "test error: 70208.33203895975\n",
      "train error: 0.2874576360086217\n",
      "accuracy: 0.8047201444415786\n",
      "Subset: 2\n",
      "test error: 8420811.02692854\n",
      "train error: 0.25997575075353124\n",
      "accuracy: 0.8294182519988972\n"
     ]
    }
   ],
   "source": [
    "jet_tX = jet(tX)\n",
    "\n",
    "lams= [1e-4,0.001,1e-5]\n",
    "\n",
    "degs = 12*np.ones(3).astype(int)\n",
    "losses=[]\n",
    "\n",
    "for i in range(len(jet_tX)):\n",
    "    # preprocess every train subset\n",
    "    preprocessed_tX = preprocess_data(tX[jet_tX[i]])\n",
    "    print(\"Subset: \" + str(i))\n",
    "    acc, testloss, trainloss, weights = cross_validation_ridge(y[jet_tX[i]], preprocessed_tX,lams[i],degs[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: 0\n",
      "test error: 0.607156524739386\n",
      "train error: 35868.314344267805\n",
      "accuracy: 0.8142728455610049\n",
      "Subset: 1\n",
      "test error: 0.49150845643121277\n",
      "train error: 35341.373236731735\n",
      "accuracy: 0.7057518700025793\n",
      "Subset: 2\n",
      "test error: 0.5511015373801456\n",
      "train error: 32326.434123875497\n",
      "accuracy: 0.7099255583126551\n"
     ]
    }
   ],
   "source": [
    "jet_tX = jet(tX)\n",
    "\n",
    "means = []\n",
    "devs = []\n",
    "degree = [1, 1, 1]\n",
    "# cleans -999 and standardizes\n",
    "for i in range(len(jet_tX)):\n",
    "    # preprocess every train subset\n",
    "    preprocessed_tX = preprocess_data(tX[jet_tX[i]])\n",
    "    print(\"Subset: \" + str(i))\n",
    "    acc, testloss, trainloss, weights = cross_validation_for_logistic(y[jet_tX[i]], preprocessed_tX, degree[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularised logistic regression implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'axis' is an invalid keyword argument for min()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0fa2ceed27e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'axis' is an invalid keyword argument for min()"
     ]
    }
   ],
   "source": [
    "#grid search for best lambdas\n",
    "jet_tX = jet(tX)\n",
    "\n",
    "means = []\n",
    "devs = []\n",
    "degree = [1,1,1]\n",
    "# cleans -999 and standardizes\n",
    "accs=[]\n",
    "lambdas=np.logspace(0,3,num=20)\n",
    "for lambda_ in lambdas:\n",
    "    for i in range(len(jet_tX)):\n",
    "        # preprocess every train subset\n",
    "        preprocessed_tX = preprocess_data(tX[jet_tX[i]])\n",
    "        acc, testloss, trainloss, weights = cross_validation_for_reglogistic(y[jet_tX[i]], preprocessed_tX,lambda_, degree[i])\n",
    "        accs.append(acc)\n",
    "accs=np.asarray(accs)\n",
    "accs.reshape(-1,3)\n",
    "print(min(accs,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[233.57214690901213, 2.976351441631318, 78.47599703514611]\n"
     ]
    }
   ],
   "source": [
    "accsme=np.mean(accs,axis=1)\n",
    "accsme=accsme.reshape(-1,3)\n",
    "goodlambdas=[lambdas[np.argmax(accsme[:,0])], lambdas[np.argmax(accsme[:,1])], lambdas[np.argmax(accsme[:,2])]]\n",
    "print(goodlambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: 0\n",
      "test error: 0.643346313135406\n",
      "train error: 36149.18962227507\n",
      "accuracy: 0.8152737463717346\n",
      "Subset: 1\n",
      "test error: 0.49151228278468817\n",
      "train error: 35342.92285417614\n",
      "accuracy: 0.7057389734330668\n",
      "Subset: 2\n",
      "test error: 0.5511914494093625\n",
      "train error: 32381.416439138622\n",
      "accuracy: 0.7106699751861042\n"
     ]
    }
   ],
   "source": [
    "#function called with the bestlambdas found in previous step\n",
    "jet_tX = jet(tX)\n",
    "\n",
    "goodlambdas= [233.57214690901213, 2.9763514416313179, 78.475997035146108]\n",
    "\n",
    "means = []\n",
    "devs = []\n",
    "degree = [1,1,1]\n",
    "# cleans -999 and standardizes\n",
    "accs=[]\n",
    "\n",
    "lambdas=goodlambdas\n",
    "\n",
    "for i in range(len(jet_tX)):\n",
    "    print(\"Subset: \" + str(i))\n",
    "    # preprocess every train subset\n",
    "    preprocessed_tX = preprocess_data(tX[jet_tX[i]])\n",
    "    acc, testloss, trainloss, weights = cross_validation_for_reglogistic(y[jet_tX[i]], preprocessed_tX,lambdas[i], degree[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8631864728562834, 0.4832607028424577, 0.49314131426161195, 0.8631873777247719, 0.4832607030257133, 0.49314131431398334, 0.8631842263651718, 0.48326070330115534, 0.4931413144520695, 0.8631856999364111, 0.4832607022911984, 0.49314131481615386, 0.8631836416254774, 0.4832607028340903, 0.4931413157761151, 0.863187178550533, 0.4832607026229851, 0.4931413183071827, 0.8631850475996731, 0.48326070391096154, 0.4931413249806259, 0.863182907667257, 0.48326070396344206, 0.49314134257547887, 0.8631828153865209, 0.48326070720978426, 0.49314138896218335, 0.8631876272593108, 0.483260714985934, 0.4931415112345318, 0.8631830796997476, 0.48326073463548824, 0.49314183339374673, 0.86318573481566, 0.4832607863565696, 0.493142681223469, 0.863184120470095, 0.4832609234871037, 0.4931449057555667, 0.8631831628658986, 0.4832612841797311, 0.4931506982039143, 0.8631892409904786, 0.4832622362335288, 0.49316551134594866, 0.8631867393985223, 0.48326474540578473, 0.493202016571479, 0.863193392293753, 0.48327135480823513, 0.4932868185063454, 0.8632071999462251, 0.4832887416848962, 0.4934683806203036, 0.8632360941301471, 0.4833343047958508, 0.4938021344123708, 0.8633169724784007, 0.4834525227600915, 0.49426197921695775, 0.8631864728562834, 0.4832607028424577, 0.49314131426161195, 0.8631873777247719, 0.4832607030257133, 0.49314131431398334, 0.8631842263651718, 0.48326070330115534, 0.4931413144520695, 0.8631856999364111, 0.4832607022911984, 0.49314131481615386, 0.8631836416254774, 0.4832607028340903, 0.4931413157761151, 0.863187178550533, 0.4832607026229851, 0.4931413183071827, 0.8631850475996731, 0.48326070391096154, 0.4931413249806259, 0.863182907667257, 0.48326070396344206, 0.49314134257547887, 0.8631828153865209, 0.48326070720978426, 0.49314138896218335, 0.8631876272593108, 0.483260714985934, 0.4931415112345318, 0.8631830796997476, 0.48326073463548824, 0.49314183339374673, 0.86318573481566, 0.4832607863565696, 0.493142681223469, 0.863184120470095, 0.4832609234871037, 0.4931449057555667, 0.8631831628658986, 0.4832612841797311, 0.4931506982039143, 0.8631892409904786, 0.4832622362335288, 0.49316551134594866, 0.8631867393985223, 0.48326474540578473, 0.493202016571479, 0.863193392293753, 0.48327135480823513, 0.4932868185063454, 0.8632071999462251, 0.4832887416848962, 0.4934683806203036, 0.8632360941301471, 0.4833343047958508, 0.4938021344123708, 0.8633169724784007, 0.4834525227600915, 0.49426197921695775, 0.8631864728562834, 0.4832607028424577, 0.49314131426161195, 0.8631873777247719, 0.4832607030257133, 0.49314131431398334, 0.8631842263651718, 0.48326070330115534, 0.4931413144520695, 0.8631856999364111, 0.4832607022911984, 0.49314131481615386, 0.8631836416254774, 0.4832607028340903, 0.4931413157761151, 0.863187178550533, 0.4832607026229851, 0.4931413183071827, 0.8631850475996731, 0.48326070391096154, 0.4931413249806259, 0.863182907667257, 0.48326070396344206, 0.49314134257547887, 0.8631828153865209, 0.48326070720978426, 0.49314138896218335, 0.8631876272593108, 0.483260714985934, 0.4931415112345318, 0.8631830796997476, 0.48326073463548824, 0.49314183339374673, 0.86318573481566, 0.4832607863565696, 0.493142681223469, 0.863184120470095, 0.4832609234871037, 0.4931449057555667, 0.8631831628658986, 0.4832612841797311, 0.4931506982039143, 0.8631892409904786, 0.4832622362335288, 0.49316551134594866, 0.8631867393985223, 0.48326474540578473, 0.493202016571479, 0.863193392293753, 0.48327135480823513, 0.4932868185063454, 0.8632071999462251, 0.4832887416848962, 0.4934683806203036, 0.8632360941301471, 0.4833343047958508, 0.4938021344123708, 0.8633169724784007, 0.4834525227600915, 0.49426197921695775, 0.8631864728562834, 0.4832607028424577, 0.49314131426161195, 0.8631873777247719, 0.4832607030257133, 0.49314131431398334, 0.8631842263651718, 0.48326070330115534, 0.4931413144520695, 0.8631856999364111, 0.4832607022911984, 0.49314131481615386, 0.8631836416254774, 0.4832607028340903, 0.4931413157761151, 0.863187178550533, 0.4832607026229851, 0.4931413183071827, 0.8631850475996731, 0.48326070391096154, 0.4931413249806259, 0.863182907667257, 0.48326070396344206, 0.49314134257547887, 0.8631828153865209, 0.48326070720978426, 0.49314138896218335, 0.8631876272593108, 0.483260714985934, 0.4931415112345318, 0.8631830796997476, 0.48326073463548824, 0.49314183339374673, 0.86318573481566, 0.4832607863565696, 0.493142681223469, 0.863184120470095, 0.4832609234871037, 0.4931449057555667, 0.8631831628658986, 0.4832612841797311, 0.4931506982039143, 0.8631892409904786, 0.4832622362335288, 0.49316551134594866, 0.8631867393985223, 0.48326474540578473, 0.493202016571479, 0.863193392293753, 0.48327135480823513, 0.4932868185063454, 0.8632071999462251, 0.4832887416848962, 0.4934683806203036, 0.8632360941301471, 0.4833343047958508, 0.4938021344123708, 0.8633169724784007, 0.4834525227600915, 0.49426197921695775, 0.8631864728562834, 0.4832607028424577, 0.49314131426161195, 0.8631873777247719, 0.4832607030257133, 0.49314131431398334, 0.8631842263651718, 0.48326070330115534, 0.4931413144520695, 0.8631856999364111, 0.4832607022911984, 0.49314131481615386, 0.8631836416254774, 0.4832607028340903, 0.4931413157761151, 0.863187178550533, 0.4832607026229851, 0.4931413183071827, 0.8631850475996731, 0.48326070391096154, 0.4931413249806259, 0.863182907667257, 0.48326070396344206, 0.49314134257547887, 0.8631828153865209, 0.48326070720978426, 0.49314138896218335, 0.8631876272593108, 0.483260714985934, 0.4931415112345318, 0.8631830796997476, 0.48326073463548824, 0.49314183339374673, 0.86318573481566, 0.4832607863565696, 0.493142681223469, 0.863184120470095, 0.4832609234871037, 0.4931449057555667, 0.8631831628658986, 0.4832612841797311, 0.4931506982039143, 0.8631892409904786, 0.4832622362335288, 0.49316551134594866, 0.8631867393985223, 0.48326474540578473, 0.493202016571479, 0.863193392293753, 0.48327135480823513, 0.4932868185063454, 0.8632071999462251, 0.4832887416848962, 0.4934683806203036, 0.8632360941301471, 0.4833343047958508, 0.4938021344123708, 0.8633169724784007, 0.4834525227600915, 0.49426197921695775, 0.8631864728562834, 0.4832607028424577, 0.49314131426161195, 0.8631873777247719, 0.4832607030257133, 0.49314131431398334, 0.8631842263651718, 0.48326070330115534, 0.4931413144520695, 0.8631856999364111, 0.4832607022911984, 0.49314131481615386, 0.8631836416254774, 0.4832607028340903, 0.4931413157761151, 0.863187178550533, 0.4832607026229851, 0.4931413183071827, 0.8631850475996731, 0.48326070391096154, 0.4931413249806259, 0.863182907667257, 0.48326070396344206, 0.49314134257547887, 0.8631828153865209, 0.48326070720978426, 0.49314138896218335, 0.8631876272593108, 0.483260714985934, 0.4931415112345318, 0.8631830796997476, 0.48326073463548824, 0.49314183339374673, 0.86318573481566, 0.4832607863565696, 0.493142681223469, 0.863184120470095, 0.4832609234871037, 0.4931449057555667, 0.8631831628658986, 0.4832612841797311, 0.4931506982039143, 0.8631892409904786, 0.4832622362335288, 0.49316551134594866, 0.8631867393985223, 0.48326474540578473, 0.493202016571479, 0.863193392293753, 0.48327135480823513, 0.4932868185063454, 0.8632071999462251, 0.4832887416848962, 0.4934683806203036, 0.8632360941301471, 0.4833343047958508, 0.4938021344123708, 0.8633169724784007, 0.4834525227600915, 0.49426197921695775, 0.8631864728562834, 0.4832607028424577, 0.49314131426161195, 0.8631873777247719, 0.4832607030257133, 0.49314131431398334, 0.8631842263651718, 0.48326070330115534, 0.4931413144520695, 0.8631856999364111, 0.4832607022911984, 0.49314131481615386, 0.8631836416254774, 0.4832607028340903, 0.4931413157761151, 0.863187178550533, 0.4832607026229851, 0.4931413183071827, 0.8631850475996731, 0.48326070391096154, 0.4931413249806259, 0.863182907667257, 0.48326070396344206, 0.49314134257547887, 0.8631828153865209, 0.48326070720978426, 0.49314138896218335, 0.8631876272593108, 0.483260714985934, 0.4931415112345318, 0.8631830796997476, 0.48326073463548824, 0.49314183339374673, 0.86318573481566, 0.4832607863565696, 0.493142681223469, 0.863184120470095, 0.4832609234871037, 0.4931449057555667, 0.8631831628658986, 0.4832612841797311, 0.4931506982039143, 0.8631892409904786, 0.4832622362335288, 0.49316551134594866, 0.8631867393985223, 0.48326474540578473, 0.493202016571479, 0.863193392293753, 0.48327135480823513, 0.4932868185063454, 0.8632071999462251, 0.4832887416848962, 0.4934683806203036, 0.8632360941301471, 0.4833343047958508, 0.4938021344123708, 0.8633169724784007, 0.4834525227600915, 0.49426197921695775, 0.8631864728562834, 0.4832607028424577, 0.49314131426161195, 0.8631873777247719, 0.4832607030257133, 0.49314131431398334, 0.8631842263651718, 0.48326070330115534, 0.4931413144520695, 0.8631856999364111, 0.4832607022911984, 0.49314131481615386, 0.8631836416254774, 0.4832607028340903, 0.4931413157761151, 0.863187178550533, 0.4832607026229851, 0.4931413183071827, 0.8631850475996731, 0.48326070391096154, 0.4931413249806259, 0.863182907667257, 0.48326070396344206, 0.49314134257547887, 0.8631828153865209, 0.48326070720978426, 0.49314138896218335, 0.8631876272593108, 0.483260714985934, 0.4931415112345318, 0.8631830796997476, 0.48326073463548824, 0.49314183339374673, 0.86318573481566, 0.4832607863565696, 0.493142681223469, 0.863184120470095, 0.4832609234871037, 0.4931449057555667, 0.8631831628658986, 0.4832612841797311, 0.4931506982039143, 0.8631892409904786, 0.4832622362335288, 0.49316551134594866, 0.8631867393985223, 0.48326474540578473, 0.493202016571479, 0.863193392293753, 0.48327135480823513, 0.4932868185063454, 0.8632071999462251, 0.4832887416848962, 0.4934683806203036, 0.8632360941301471, 0.4833343047958508, 0.4938021344123708, 0.8633169724784007, 0.4834525227600915, 0.49426197921695775, 0.8631864728562834, 0.4832607028424577, 0.49314131426161195, 0.8631873777247719, 0.4832607030257133, 0.49314131431398334, 0.8631842263651718, 0.48326070330115534, 0.4931413144520695, 0.8631856999364111, 0.4832607022911984, 0.49314131481615386, 0.8631836416254774, 0.4832607028340903, 0.4931413157761151, 0.863187178550533, 0.4832607026229851, 0.4931413183071827, 0.8631850475996731, 0.48326070391096154, 0.4931413249806259, 0.863182907667257, 0.48326070396344206, 0.49314134257547887, 0.8631828153865209, 0.48326070720978426, 0.49314138896218335, 0.8631876272593108, 0.483260714985934, 0.4931415112345318, 0.8631830796997476, 0.48326073463548824, 0.49314183339374673, 0.86318573481566, 0.4832607863565696, 0.493142681223469, 0.863184120470095, 0.4832609234871037, 0.4931449057555667, 0.8631831628658986, 0.4832612841797311, 0.4931506982039143, 0.8631892409904786, 0.4832622362335288, 0.49316551134594866, 0.8631867393985223, 0.48326474540578473, 0.493202016571479, 0.863193392293753, 0.48327135480823513, 0.4932868185063454, 0.8632071999462251, 0.4832887416848962, 0.4934683806203036, 0.8632360941301471, 0.4833343047958508, 0.4938021344123708, 0.8633169724784007, 0.4834525227600915, 0.49426197921695775, 0.8631864728562834, 0.4832607028424577, 0.49314131426161195, 0.8631873777247719, 0.4832607030257133, 0.49314131431398334, 0.8631842263651718, 0.48326070330115534, 0.4931413144520695, 0.8631856999364111, 0.4832607022911984, 0.49314131481615386, 0.8631836416254774, 0.4832607028340903, 0.4931413157761151, 0.863187178550533, 0.4832607026229851, 0.4931413183071827, 0.8631850475996731, 0.48326070391096154, 0.4931413249806259, 0.863182907667257, 0.48326070396344206, 0.49314134257547887, 0.8631828153865209, 0.48326070720978426, 0.49314138896218335, 0.8631876272593108, 0.483260714985934, 0.4931415112345318, 0.8631830796997476, 0.48326073463548824, 0.49314183339374673, 0.86318573481566, 0.4832607863565696, 0.493142681223469, 0.863184120470095, 0.4832609234871037, 0.4931449057555667, 0.8631831628658986, 0.4832612841797311, 0.4931506982039143, 0.8631892409904786, 0.4832622362335288, 0.49316551134594866, 0.8631867393985223, 0.48326474540578473, 0.493202016571479, 0.863193392293753, 0.48327135480823513, 0.4932868185063454, 0.8632071999462251, 0.4832887416848962, 0.4934683806203036, 0.8632360941301471, 0.4833343047958508, 0.4938021344123708, 0.8633169724784007, 0.4834525227600915, 0.49426197921695775, 0.8631864728562834, 0.4832607028424577, 0.49314131426161195, 0.8631873777247719, 0.4832607030257133, 0.49314131431398334, 0.8631842263651718, 0.48326070330115534, 0.4931413144520695, 0.8631856999364111, 0.4832607022911984, 0.49314131481615386, 0.8631836416254774, 0.4832607028340903, 0.4931413157761151, 0.863187178550533, 0.4832607026229851, 0.4931413183071827, 0.8631850475996731, 0.48326070391096154, 0.4931413249806259, 0.863182907667257, 0.48326070396344206, 0.49314134257547887, 0.8631828153865209, 0.48326070720978426, 0.49314138896218335, 0.8631876272593108, 0.483260714985934, 0.4931415112345318, 0.8631830796997476, 0.48326073463548824, 0.49314183339374673, 0.86318573481566, 0.4832607863565696, 0.493142681223469, 0.863184120470095, 0.4832609234871037, 0.4931449057555667, 0.8631831628658986, 0.4832612841797311, 0.4931506982039143, 0.8631892409904786, 0.4832622362335288, 0.49316551134594866, 0.8631867393985223, 0.48326474540578473, 0.493202016571479, 0.863193392293753, 0.48327135480823513, 0.4932868185063454, 0.8632071999462251, 0.4832887416848962, 0.4934683806203036, 0.8632360941301471, 0.4833343047958508, 0.4938021344123708, 0.8633169724784007, 0.4834525227600915, 0.49426197921695775, 0.8631864728562834, 0.4832607028424577, 0.49314131426161195, 0.8631873777247719, 0.4832607030257133, 0.49314131431398334, 0.8631842263651718, 0.48326070330115534, 0.4931413144520695, 0.8631856999364111, 0.4832607022911984, 0.49314131481615386, 0.8631836416254774, 0.4832607028340903, 0.4931413157761151, 0.863187178550533, 0.4832607026229851, 0.4931413183071827, 0.8631850475996731, 0.48326070391096154, 0.4931413249806259, 0.863182907667257, 0.48326070396344206, 0.49314134257547887, 0.8631828153865209, 0.48326070720978426, 0.49314138896218335, 0.8631876272593108, 0.483260714985934, 0.4931415112345318, 0.8631830796997476, 0.48326073463548824, 0.49314183339374673, 0.86318573481566, 0.4832607863565696, 0.493142681223469, 0.863184120470095, 0.4832609234871037, 0.4931449057555667, 0.8631831628658986, 0.4832612841797311, 0.4931506982039143, 0.8631892409904786, 0.4832622362335288, 0.49316551134594866, 0.8631867393985223, 0.48326474540578473, 0.493202016571479, 0.863193392293753, 0.48327135480823513, 0.4932868185063454, 0.8632071999462251, 0.4832887416848962, 0.4934683806203036, 0.8632360941301471, 0.4833343047958508, 0.4938021344123708, 0.8633169724784007, 0.4834525227600915, 0.49426197921695775]\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission_splitt.csv'\n",
    "create_csv_submission(ids_test, y_preds, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial expansion of test data\n",
    "#tX_testpol=build_poly(tX_test,degree)\n",
    "\n",
    "OUTPUT_PATH = '../data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
